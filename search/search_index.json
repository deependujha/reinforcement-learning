{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Reinforcement Learning","text":""},{"location":"01-topics-to-be-covered/","title":"Topics to be covered","text":"<ul> <li>Markov Decision Processes &amp; Planning</li> <li>Model-free Evaluation</li> <li>Model-free Control</li> <li>Policy Search</li> <li>Offline RL including RL from human feedback and Direct Preference Optimization</li> <li>Exploration</li> <li>Advanced Topics</li> </ul> <p>Offline RL?</p> <p>Offline RL: Limited data</p>"},{"location":"01-topics-to-be-covered/#plan-of-action","title":"Plan of action","text":"<ol> <li>Model-free:<ul> <li>Q-learning</li> <li>Policy Gradient</li> <li>Actor-Critic</li> </ul> </li> <li>Model-based:<ul> <li>Planning</li> <li>Sequence models</li> </ul> </li> <li>Exploration</li> <li>Offline RL</li> <li>Inverse Reinforcement Learning</li> <li>Meta Learning</li> <li>Transfer Learning</li> <li>Multi-agent RL</li> </ol>"},{"location":"01-topics-to-be-covered/#homeworks-assignments","title":"Homeworks &amp; assignments","text":"<ol> <li>HW1: Imitation Learning (control via supervised learning)</li> <li>HW2: Policy Gradient</li> <li>HW3: Q-learning &amp; actor-critic algorithms</li> <li>HW4: Model-based RL</li> <li>HW5: Offline RL</li> <li>Final Project: Research level project of your choice</li> </ol>"},{"location":"02-maths/","title":"Maths in Markdown","text":""},{"location":"02-maths/#an-example","title":"An example","text":"<ul> <li>The Cauchy-Schwarz Inequality</li> </ul> \\[\\left( \\sum_{k=1}^n a_k b_k \\right)^2 \\leq \\left( \\sum_{k=1}^n a_k^2 \\right) \\left( \\sum_{k=1}^n b_k^2 \\right)\\]"},{"location":"02-maths/#basic-maths","title":"Basic maths","text":"Markdown Syntax Math Expression <code>a + b</code> \\( a + b \\) (Plus) <code>a - b</code> \\( a - b \\) (Minus) <code>a \\times b</code> \\( a \\times b \\) (Multiplication) <code>a \\cdot b</code> \\( a \\cdot b \\) (Dot product) <code>\\frac{a}{b}</code> \\( \\frac{a}{b} \\) (Fraction) <code>a \\div b</code> \\( a \\div b \\) (Division) <code>a \\pm b</code> \\( a \\pm b \\) (Plus-minus) <code>a \\mp b</code> \\( a \\mp b \\) (Minus-plus) <code>a \\ast b</code> \\( a \\ast b \\) (Asterisk multiplication) <code>a \\oplus b</code> \\( a \\oplus b \\) (XOR / Direct sum) <code>a \\otimes b</code> \\( a \\otimes b \\) (Tensor product)"},{"location":"02-maths/#parentheses","title":"Parentheses","text":"Markdown Syntax Math Expression <code>(a+b)</code> \\( (a+b) \\) <code>[a+b]</code> \\( [a+b] \\) <code>\\{a+b\\}</code> \\( \\{a+b\\} \\) <code>\\langle a+b \\rangle</code> \\( \\langle a+b \\rangle \\) <code>\\lfloor a \\rfloor</code> \\( \\lfloor a \\rfloor \\) (Floor function) <code>\\lceil a \\rceil</code> \\( \\lceil a \\rceil \\) (Ceiling function) <p>For scalable parentheses (auto-sized based on content):</p> <pre><code>\\left( \\frac{a}{b} \\right)\n</code></pre> \\[ \\left( \\frac{a}{b} \\right) \\]"},{"location":"02-maths/#common-math-symbols","title":"Common math symbols","text":"Markdown Syntax Math Expression <code>\\sigma</code> \\( \\sigma \\) <code>a^b</code> \\( a^b \\) <code>\\sqrt{x}</code> \\( \\sqrt{x} \\) <code>\\log x</code> \\( \\log x \\) <code>\\log_{y} x</code> \\( \\log_{y} x \\) <code>\\ln x</code> \\( \\ln x \\) <code>e^x</code> \\( e^x \\) <code>\\pi</code> \\( \\pi \\) <code>\\int f(x) \\,dx</code> \\( \\int f(x) \\,dx \\) <code>\\frac{d}{dx} f(x)</code> \\( \\frac{d}{dx} f(x) \\) <code>\\frac{\\partial f}{\\partial x}</code> \\( \\frac{\\partial f}{\\partial x} \\) <code>\\iint f(x, y) \\,dx\\,dy</code> \\( \\iint f(x, y) \\,dx\\,dy \\) <code>\\iiint f(x, y, z) \\,dx\\,dy\\,dz</code> \\( \\iiint f(x, y, z) \\,dx\\,dy\\,dz \\) <code>\\oint f(x) \\,dx</code> \\( \\oint f(x) \\,dx \\) <code>\\sin x</code> \\( \\sin x \\) <code>\\cos x</code> \\( \\cos x \\) <code>\\tan x</code> \\( \\tan x \\) <code>\\sinh x</code> \\( \\sinh x \\) <code>\\cosh x</code> \\( \\cosh x \\) <code>\\tanh x</code> \\( \\tanh x \\) <code>\\Delta x</code> \\( \\Delta x \\) <code>\\lambda</code> \\( \\lambda \\) <code>\\sum_{i=1}^{n} x_i</code> \\( \\sum_{i=1}^{n} x_i \\) <code>\\prod_{i=1}^{n} x_i</code> \\( \\prod_{i=1}^{n} x_i \\) <code>\\lim_{x \\to a} f(x)</code> \\( \\lim_{x \\to a} f(x) \\) <code>\\approx</code> \\( \\approx \\) <code>\\neq</code> \\( \\neq \\) <code>\\geq</code> \\( \\geq \\) <code>\\leq</code> \\( \\leq \\) <code>\\inf</code> \\( \\inf \\) <code>\\infty</code> \\( \\infty \\) <code>\\max</code> \\( \\max \\) <code>\\min</code> \\( \\min \\) <code>\\arg \\max</code> \\( \\arg \\max \\) <code>\\arg \\min</code> \\( \\arg \\min \\)"},{"location":"02-maths/#matrix","title":"Matrix","text":"<p>Here\u2019s how to write rows, columns, and matrices in LaTeX (Markdown math mode):</p>"},{"location":"02-maths/#1-row-vector","title":"1. Row Vector","text":"<pre><code>\\begin{bmatrix} a &amp; b &amp; c \\end{bmatrix}\n</code></pre> \\[ \\begin{bmatrix} a &amp; b &amp; c \\end{bmatrix} \\]"},{"location":"02-maths/#2-column-vector","title":"2. Column Vector","text":"<pre><code>\\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix}\n</code></pre> \\[ \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} \\]"},{"location":"02-maths/#3-matrix-general-form","title":"3. Matrix (General Form)","text":"<pre><code>\\begin{bmatrix} \na &amp; b \\\\ \nc &amp; d \n\\end{bmatrix}\n</code></pre> \\[ \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix} \\]"},{"location":"02-maths/#4-identity-matrix-33-example","title":"4. Identity Matrix (3\u00d73 example)","text":"<pre><code>\\begin{bmatrix}\n1 &amp; 0 &amp; 0 \\\\\n0 &amp; 1 &amp; 0 \\\\\n0 &amp; 0 &amp; 1\n\\end{bmatrix}\n</code></pre> \\[ \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\]"},{"location":"02-maths/#probability","title":"Probability","text":"<p>Here\u2019s a table of probability-related symbols in Markdown (LaTeX math mode):  </p> Markdown Syntax Math Expression <code>P(A)</code> \\( P(A) \\) (Probability of event A) <code>\\mathbb{P}(A)</code> \\( \\mathbb{P}(A) \\) (Alternative notation for probability) <code>P(A \\cap B)</code> \\( P(A \\cap B) \\) (Probability of A and B) <code>P(A \\cup B)</code> \\( P(A \\cup B) \\) (Probability of A or B) <code>P(A \\vert B)</code> \\( P(A \\vert B) \\) (Conditional probability) <code>P(A \\mid B)</code> \\( P(A \\mid B) \\) (Alternative conditional probability notation) <code>\\mathbb{E}[X]</code> \\( \\mathbb{E}[X] \\) (Expected value of X) <code>\\operatorname{Var}(X)</code> \\( \\operatorname{Var}(X) \\) (Variance of X) <code>\\operatorname{Cov}(X, Y)</code> \\( \\operatorname{Cov}(X, Y) \\) (Covariance of X and Y) <code>X \\sim \\mathcal{N}(\\mu, \\sigma^2)</code> \\( X \\sim \\mathcal{N}(\\mu, \\sigma^2) \\) (X follows a normal distribution) <code>X \\sim \\text{Bern}(p)</code> \\( X \\sim \\text{Bern}(p) \\) (X follows a Bernoulli distribution) <code>X \\sim \\text{Bin}(n, p)</code> \\( X \\sim \\text{Bin}(n, p) \\) (X follows a Binomial distribution) <code>X \\sim \\text{Poisson}(\\lambda)</code> \\( X \\sim \\text{Poisson}(\\lambda) \\) (X follows a Poisson distribution) <code>H(X) = -\\sum p(x) \\log p(x)</code> \\( H(X) = -\\sum p(x) \\log p(x) \\) (Entropy formula)"},{"location":"02-maths/#greek-letters","title":"Greek Letters","text":"Markdown Syntax Math Expression <code>\\alpha</code> \\( \\alpha \\) <code>\\beta</code> \\( \\beta \\) <code>\\gamma</code> \\( \\gamma \\) <code>\\delta</code> \\( \\delta \\) <code>\\epsilon</code> \\( \\epsilon \\) <code>\\zeta</code> \\( \\zeta \\) <code>\\eta</code> \\( \\eta \\) <code>\\theta</code> \\( \\theta \\) <code>\\iota</code> \\( \\iota \\) <code>\\kappa</code> \\( \\kappa \\) <code>\\lambda</code> \\( \\lambda \\) <code>\\mu</code> \\( \\mu \\) <code>\\nu</code> \\( \\nu \\) <code>\\xi</code> \\( \\xi \\) <code>\\pi</code> \\( \\pi \\) <code>\\rho</code> \\( \\rho \\) <code>\\sigma</code> \\( \\sigma \\) <code>\\tau</code> \\( \\tau \\) <code>\\upsilon</code> \\( \\upsilon \\) <code>\\phi</code> \\( \\phi \\) <code>\\chi</code> \\( \\chi \\) <code>\\psi</code> \\( \\psi \\) <code>\\omega</code> \\( \\omega \\) <p>Capital Greek letters</p> <p>For uppercase versions, just capitalize the first letter, e.g., <code>\\Delta</code> \u2192 \\( \\Delta \\), <code>\\Gamma</code> \u2192 \\( \\Gamma \\).  </p>"},{"location":"02-maths/#calculus","title":"Calculus","text":"Markdown Syntax Math Expression <code>\\frac{a}{b}</code> \\( \\frac{a}{b} \\) (Fraction) <code>\\sum_{i=1}^{n} x_i</code> \\( \\sum_{i=1}^{n} x_i \\) (Summation) <code>\\prod_{i=1}^{n} x_i</code> \\( \\prod_{i=1}^{n} x_i \\) (Product notation) <code>\\int_a^b f(x) \\,dx</code> \\( \\int_a^b f(x) \\,dx \\) (Definite Integral) <code>\\int f(x) \\,dx</code> \\( \\int f(x) \\,dx \\) (Indefinite Integral) <code>\\iint f(x, y) \\,dx \\,dy</code> \\( \\iint f(x, y) \\,dx \\,dy \\) (Double Integral) <code>\\iiint f(x, y, z) \\,dx \\,dy \\,dz</code> \\( \\iiint f(x, y, z) \\,dx \\,dy \\,dz \\) (Triple Integral) <code>\\oint f(x) \\,dx</code> \\( \\oint f(x) \\,dx \\) (Contour Integral) <code>\\frac{d}{dx} f(x)</code> \\( \\frac{d}{dx} f(x) \\) (Derivative) <code>\\frac{\\partial}{\\partial x} f(x,y)</code> \\( \\frac{\\partial}{\\partial x} f(x,y) \\) (Partial Derivative) <code>\\nabla f</code> \\( \\nabla f \\) (Gradient) <code>\\lim_{x \\to a} f(x)</code> \\( \\lim_{x \\to a} f(x) \\) (Limit)"},{"location":"02-maths/#some-more-symbols","title":"Some more symbols","text":"<p>Here are some more symbols like the cap, hat, overline, and similar:  </p>"},{"location":"02-maths/#1-overhead-symbols","title":"1. Overhead Symbols","text":"Markdown Syntax Rendered Output Usage <code>\\hat{x}</code> \\( \\hat{x} \\) Estimators, unit vectors <code>\\widehat{xyz}</code> \\( \\widehat{xyz} \\) Longer expressions <code>\\bar{x}</code> \\( \\bar{x} \\) Mean, averages <code>\\overline{ABC}</code> \\( \\overline{ABC} \\) Complex conjugates, closures <code>\\tilde{x}</code> \\( \\tilde{x} \\) Approximation, modified variables <code>\\widetilde{XYZ}</code> \\( \\widetilde{XYZ} \\) Extended tilde notation"},{"location":"02-maths/#2-set-and-logic-symbols","title":"2. Set and Logic Symbols","text":"Markdown Syntax Rendered Output Usage <code>A \\cup B</code> \\( A \\cup B \\) Union of sets <code>A \\cap B</code> \\( A \\cap B \\) Intersection of sets <code>A \\subset B</code> \\( A \\subset B \\) Subset <code>A \\supset B</code> \\( A \\supset B \\) Superset <code>A \\subseteq B</code> \\( A \\subseteq B \\) Subset or equal <code>A \\supseteq B</code> \\( A \\supseteq B \\) Superset or equal"},{"location":"02-maths/#3-special-notation","title":"3. Special Notation","text":"Markdown Syntax Rendered Output Usage <code>\\vee</code> \\( \\vee \\) Logical OR <code>\\wedge</code> \\( \\wedge \\) Logical AND <code>\\neg P</code> \\( \\neg P \\) Logical NOT <code>\\forall x</code> \\( \\forall x \\) \"For all\" (universal quantifier) <code>\\exists y</code> \\( \\exists y \\) \"There exists\" (existential quantifier) <code>\\nexists z</code> \\( \\nexists z \\) \"There does not exist\" <code>\\Rightarrow</code> \\( \\Rightarrow \\) Implies <code>\\Leftrightarrow</code> \\( \\Leftrightarrow \\) If and only if"},{"location":"01-intro/01-immediate-rl-and-bandits/","title":"Immediate RL &amp; Bandits","text":"<p>In many scenarios, the action that we take don't show their effect immediately, but over time. Like when you lose in chess, which move caused you to lose might not be the last move you made. Or, when you fall of your bike, you don't know which action caused you to fall off.</p> <p>But, In many cases, the <code>action we take show their effect immediately</code>.</p> <p>Like when you play a slot machine, you know exactly how much you won or lost. Each lever that you pull is an action, and the outcome is the reward that you get immediately. And, it doesn't effect later pulls.</p> <p>Such problems are called <code>Immediate RL</code> or <code>Multi-Armed Bandits</code> problems.</p> <p></p>"},{"location":"01-intro/01-immediate-rl-and-bandits/#exploration-vs-exploitation","title":"Exploration vs exploitation","text":"<ul> <li>Exploration: Trying out new actions to find out which ones are good.</li> <li>Exploitation: Doing the action that gives the best reward.</li> </ul> <p>Explore-Exploit dilemma</p> <ul> <li>Explore to find profitable actions.</li> <li>Exploit to act according to the best observations already made.</li> <li>Always exploit might not be optimal.</li> <li>Always explore might not be optimal either.</li> <li>Hence, there is an <code>Explore-Exploit</code> dilemma.</li> </ul>"},{"location":"01-intro/01-immediate-rl-and-bandits/#bandit-problem","title":"Bandit problem","text":"<p>Bandit problem</p> <p>A <code>Bandit</code> is a problem where the action we take show their effect immediately.</p>"},{"location":"01-intro/01-immediate-rl-and-bandits/#traditional-approaches","title":"Traditional approaches","text":"<p>Let \\(r_{i,k}\\) be the reward sample acquired when \\(i^{th}\\) action is selected for the \\(k^{th}\\) time.</p> <p>Let \\(Q_i(t)\\) be the estimated value of the \\(i^{th}\\) action at time \\(t\\).</p> \\[ Q_i(t) = \\frac{\\sum_{k=1}^{t-1} r_{i,k}}{\\sum_{k:r_i, k} 1} \\] <p>Above formula is simply the average reward of the \\(i^{th}\\) action.</p> <ul> <li>Optimal action is the one with the maximum reward.</li> </ul> \\[ Q (a^{*}) = \\max_{i} \\{ Q ( a_{i} ) \\} \\] <p>We can also write <code>estimated value</code> as:</p> \\[ Q_{k+1} (a_{i}) = Q_{k} (a_{i}) + \\alpha (r_{i,k+1} - Q_{k} (a_{i})) \\] <p>Setting \\(\\alpha = \\frac{1}{k+1}\\) we get <code>average reward</code>.</p> <p>Proof: </p>"},{"location":"01-intro/01-immediate-rl-and-bandits/#exploration-approach-choosing-optimal-action","title":"Exploration approach: Choosing optimal action","text":"<p>Let's say we have 2 actions:</p> <ul> <li><code>Action 1</code>: +1 reward with 0.8 probability, 0 reward with 0.2 probability.</li> <li><code>Action 2</code>: +1 reward with 0.6 probability, 0 reward with 0.4 probability.</li> </ul> <p>If we first choose <code>Action 2</code> and got <code>+1</code> reward, then we should choose <code>Action 2</code> again.</p> <p>Action-reward table looks like this:</p> Action Last Reward Avg reward 1 0 0 2 1 1 <ul> <li>Now, since action 2 has max avg reward, we should choose it again. And, get 0 reward.</li> </ul> <p>Action-reward table looks like this:</p> Action Last Reward Avg reward 1 0 0 2 0 0.5 <ul> <li>Now, even though last reward was 0, action 2 still has the max avg reward. So we will keep exploiting it for very very long time until avg reaches 0.</li> </ul> <p>This is the <code>problem with always exploiting (greedy)</code>. If you get stuck with a suboptimal arm, you'll be in it for a very long time.</p> <p>So, we need to explore more.</p>"},{"location":"01-intro/01-immediate-rl-and-bandits/#epsilon-greedy","title":"Epsilon-Greedy","text":"<p>Select arm \\(a^* = \\arg \\max \\{ Q_k(a_i) \\}\\) with probability \\(1-\\epsilon\\) and select any arbitrary arm with probability  \\(\\epsilon\\).</p> <p></p> <pre><code>p = random()\n\nif p &lt; epsilon:\n    pull random action\nelse:\n    pull current-best action\n</code></pre> <p>Current-best arm choosing probability</p> <p>We choose current-best arm with prob: \\(1-\\epsilon\\), and then again with \\(\\epsilon \\ast \\frac{1}{N}\\).</p> \\[ \\mathbb{P}_{currentBest} = (1-\\epsilon) + ( \\epsilon \\ast \\frac{1}{N}) \\]"},{"location":"01-intro/01-immediate-rl-and-bandits/#softmax-boltzmann-exploration","title":"Softmax (<code>Boltzmann exploration</code>)","text":"<p>Select arms with probability proportional to the current value estimates.</p> \\[ \\pi (a_i) = \\frac{exp(Q_k(a_i) / \\tau)}{\\sum_{j}{exp(Q_k(a_j) / \\tau)}} \\] <ul> <li>Thanks to softmax, even when <code>Q-value (action value)</code> is negative, we can take softmax and use that as probability distribution to choose next action.</li> <li>\\(\\tau\\) is <code>temperature</code>.</li> <li>If \\(\\tau\\) is very very large number (let's say 1 billion), all the (Q-value/\\(\\tau\\)) will move to 0, making the probability distribution <code>normal distribution</code>.</li> <li>If \\(\\tau\\) is very small, (let's say 0.001), then even small differences will be blown up, and it will act like greedy.</li> </ul> <p>Both <code>epsilon-greedy</code> and <code>softmax</code> give asymptotic convergence. They both eventually reach the best arm.</p>"}]}